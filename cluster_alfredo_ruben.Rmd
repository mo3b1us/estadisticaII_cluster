---
title: "cluster_mayo"
author: "Rubén Sierra Serrano"
date: "2023-05-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Lectura y análsis de datos

En primer lugar tenemos que importar la base de datos y analizar las variables originales. 

En esta ocasión analizaremos los casos de covid de los países

```{r cars}
datos <- read.csv("Mall_Customers.csv")
summary(datos)
```

```{r}
datos <- as.data.frame(datos)
rownames(datos) <- datos$CustomerID
datos <- datos[,-1]
head(datos)
```
Comprobamos si hay datos Na en la base de datos para limpiarla
```{r}
library(inspectdf)
show_plot(inspect_na(datos))
```
Vamos a factorizar la variable Gender para poder trabajar con ella
\begin{itemize}
\item Male toma el valor 0
\item Female toma el valor 1
\end{itemize}
```{r}
datos$Gender <- ifelse(datos$Gender == "Male", 0, 1)
```

Observamos que no hay datos Na, por tanto, no es necesario tomar ninguna medida en este aspecto.
```{r}
show_plot(inspect_num(datos[,c(1:4)]))
```

```{r}
library(corrplot)
cor <- cor(datos)
corrplot.mixed(cor)
```
# Análisis de componentes principales.
Queremos reducir el número de variables de la base de datos sobre las provincias
para simplificar el análisis y, además, reducir la relación entre variables.
```{r}
# Cargamos las librerías necesarias para el análisis
library(pastecs)
library(factoextra) 
library(FactoMineR)
```

```{r}
# Análisis de componentes principales 

componentes <- PCA(datos,
                   scale.unit = TRUE, # Normalizamos/estandarizamos los datos
                   ncp = 4,          # Creamos todas las componentes posibles
                   graph = FALSE)     # No queremos graficar los resultados
```

```{r}
autovalores <- componentes$eig
autovalores
```
Vemos como con este comando obtenemos tanto los autovalores ordenados de mayor a 
menor, como el porcentaje de varianza que acumula cada uno con respecto a las 
variables originales, es decir, el porcentaje de información original que 
almacena cada una de las componentes. Además, nos muestra la varianza acumulada, 
con la que más tarde podremos tomar decisiones. 

Vamos a comprobar la variabilidad de las componentes con un gráfico.

```{r}
fviz_eig(componentes,addlabels=TRUE)
```
También podemos comporbar las componentes o nuevas variables que hemos obtenido. 
Vamos a mostrar solo las primeras observaciones.

```{r}
cp <- componentes$svd$U # Componentes principales
head(data.frame(cp))
```

Por último, podemos comprobar la varianza explicada por las componentes principales 
para cada una de las variables originales. Esto es lo mismo que comprobar la información
representada de cada variable original en cada componente.
```{r}
cos2 <- componentes$var$cos2[,c(1:4)] # Muestro solo las 5 primeras componentes
corrplot(t(cos2),method='number')
```
```{r}
fviz_cos2(componentes,choice="var",axes=1:2)
```

```{r}
fviz_pca_biplot(componentes,   
                axes = c(1,2), # Componentes 1 y 2
                label = "none") 
```

**Criterios para elegir el número óptimo de componentes.**

# Análisis de Cluster.

Una vez hemos reducido la correlación entre variables (componentes) y tenemos
tantas variables como deseamos que explican las características originales de los 
datos, comenzamos con la clasificación. 

```{r}
# Librerías para Cluster
library(cluster);
library(ggplot2);
library(heatmaply); 
library(factoextra);
library(FactoMineR); 
library(NbClust);
```

Con este análisis lo que buscamos es hacer grupos de cliente tan parecidos como
sea posible en función de las características enunciadas anteriorimente. Se tendrán
en cuenta las dos primeras componentes principales, únicamente, por lo que podemos
crear nuestra nueva base de datos en base a ellas. 

```{r}
bbdd <- data.frame(componentes$svd$U[,c(1,2)])
rownames(bbdd) <- rownames(datos)
head(bbdd)
```

## Método jerárquico.

Utilizamos los datos estandarizados y la distancia de Minkowski que es de la siguiente manera: 

$$d = (\sum_{i=1}^{4} |x_{i}-y_{i}|^{4})^{\frac{1}{4}}$$
```{r}
distancias <- dist(scale(datos), method="minkowski")
fviz_dist(distancias, show_labels = FALSE)
```

```{r}
ward <- hclust(distancias,method="ward.D2") # Algoritmo de cluster
fviz_dend(ward,cex = 0.02) # Representamos el dendograma
```

Podemos observar en el dendograma y en la prueba del codo que el número óptimo de clusteres ha emplear son 3 debido a la distancia en la que se unen los dos grandes bloques de la derecha. También podemos observar que prácticamente todas las observaciones se han unido con otra a una distancia casi nula.

```{r}
fviz_dend(ward, k = 3, cex = 0.02) 
```

Compruebo el número de observaciones que tiene cada uno de los grupos. 
```{r}
grupos <- cutree(ward,k=3)
table(grupos)

# Añado el grupo al que pertenece cada observación en la bbdd
datos$cluster1 <- grupos
head(datos)
```

A continuación, para tener una imagen visual de cómo se distribuyen los grupos en
el plano, representamos los clústeres en el plano de las dos primeras componentes principales. 
Con el análisis que hemos realizado a lo largo del trabajo, sabemos que las dos 
primeras componentes no representan el 100% de las variables, pero sí el 63.7%.

```{r}
fviz_cluster(list(data=scale(datos), cluster=grupos),
             ellipse.type="convex",
             labelsize = 0,
             show.clust.cent = TRUE, # Muestra el centroide de cada cluster
             ggtheme=theme_minimal())
```